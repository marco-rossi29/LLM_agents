{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os, json, time\n",
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "\n",
    "from pdfminer.high_level import extract_text\n",
    "from docx import Document\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "from openai import OpenAI\n",
    "from io import StringIO\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "pdf_path = \"ADD_PATH_TO_FILE\"\n",
    "word_template_path = \"ADD_PATH_OF_TEMPLATE\"\n",
    "word_output_path = \"ADD_PATH_OF_OUTPUT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test OpenAI API with a simple response generation\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    instructions=\"You are helping to extract specific fields from the \",\n",
    "    input=\"How do I check if a Python object is an instance of a class?\",\n",
    ")\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm_model(instructions, input_text, entry_max_length):\n",
    "\n",
    "    print(len(input_text))\n",
    "\n",
    "    responses_list = []\n",
    "\n",
    "    req_window = min(len(input_text), 40000)\n",
    "\n",
    "    index_end = req_window\n",
    "    while True:\n",
    "\n",
    "        index_start = max(index_end-req_window-entry_max_length, 0)\n",
    "\n",
    "        print(f\"Processing from index {index_start} to {index_end}...\")\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                response = client.responses.create(\n",
    "                    model=\"gpt-4o-2024-08-06\", #\"gpt-4o-mini-2024-07-18\",\n",
    "                    instructions=instructions,\n",
    "                    input=input_text[index_start:index_end]\n",
    "                )\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            if len(response.output_text) > 100:\n",
    "                break\n",
    "            else:\n",
    "                print('error', response)\n",
    "                print('retry in 3 seconds')\n",
    "                time.sleep(3)\n",
    "\n",
    "\n",
    "        responses_list.append(response.output_text)\n",
    "\n",
    "        if len(input_text) <= index_end:\n",
    "            print(\"Reached the end of the text.\")\n",
    "            break\n",
    "\n",
    "        index_end += req_window\n",
    "\n",
    "    return responses_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_text = extract_text(pdf_path)#, page_numbers=range(0,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_widget = widgets.Textarea(value=pdf_text, layout={'width': '100%', 'height': '600px'})\n",
    "display(text_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_cleaned = text_widget.value  # Captures human-edited version\n",
    "\n",
    "pdf_sections = pdf_cleaned.split('FINE SEZIONE')\n",
    "\n",
    "assert len(pdf_sections) == 3, \"Expected 3 sections in the PDF text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the instructions and prompt for the API\n",
    "instructions = (\n",
    "    \"You are a text-processing assistant. Your task is to parse text. You have a list of individuals [indicated by a running index 1), 2), 3) or 1. 2. 3.] with their anagraphical information;\"\n",
    "    \"Extract each individual's information in this format (index, the page of pdf in which the individual data is, name, aliases, place of birth, birth date, residence).\"\n",
    "    \"If an entry has the field 'stralcio' o 'posizione stralciata' then make 'posizione stralciata' the name and put NA in the other fields.\"\n",
    "    \"The output should be a JSON that I can import in Pandas with: pd.read_json(response.output_text.strip(), orient='records'). \"\n",
    "    \"Ignore any irrelevant content such as headers, footers, and introductory text. If any field is missing, use NA for that field. \"\n",
    "    )\n",
    "\n",
    "input_text = pdf_sections[0]\n",
    "\n",
    "responses_names = call_llm_model(instructions, input_text, entry_max_length=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [pd.read_json(StringIO(x.replace('```json', '').replace('```', '').strip()), orient='records', lines=True) for x in responses_names if x.strip()]\n",
    "\n",
    "df_names = pd.concat(dfs, ignore_index=True).drop_duplicates(subset=['index']).sort_values(by='index').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_widget = widgets.Textarea(value=pdf_sections[0], layout={'width': '100%', 'height': '600px'})\n",
    "display(text_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_widget = widgets.Textarea(value=pdf_sections[1], layout={'width': '100%', 'height': '600px'})\n",
    "display(text_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the instructions and prompt for the API\n",
    "instructions = (\n",
    "    \"You are a text-processing assistant. Your task is to parse text. The input text is a list of crimes [indicated by a running index 1), 2), 3) or 1. 2. 3., or A), B), C)], \"\n",
    "    \"each committed by a subset of the individuals. For a crime I'm interested to know if it's sancioned by 416.bis law.\"\n",
    "    \"Extract each crime's information in this format (index, the page of pdf in which the crime data is, column named '416_bis' to flag if the crime is sanctioned by the 416.bis law\"\n",
    "    \"[fill this column with either Yes or No], list of individuals that committed this crime).\"\n",
    "    \"Ensure that all the crimes are parsed out, even if they are not 416.bis. \"\n",
    "    \"The output should be a JSON that I can import in Pandas with: pd.read_json(response.output_text.strip(), orient='records'). \"\n",
    "    \"Ignore any irrelevant content such as headers, footers, and introductory text. If any field is missing, use NA for that field. \"\n",
    "    )\n",
    "\n",
    "# \"This is the list of individuals: \" + df_names['name'].str.cat(sep=', ') + \". \n",
    "# print(len(pdf_sections[1]))\n",
    "\n",
    "input_text = pdf_sections[1]\n",
    "\n",
    "responses_crimes = call_llm_model(instructions, input_text, entry_max_length=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs2 = [pd.read_json(StringIO(x.replace('```json', '').replace('```', '').strip()), orient='records') for x in responses_crimes if x and x.strip()]\n",
    "\n",
    "df_crimes = pd.concat(dfs2, ignore_index=True).drop_duplicates(subset=['index']).sort_values(by='index').reset_index(drop=True)\n",
    "df_crimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_widget = widgets.Textarea(value=pdf_sections[1], layout={'width': '100%', 'height': '500px'})\n",
    "display(text_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter only 416.bis crimes\n",
    "df_416bis = df_crimes[df_crimes[\"416_bis\"].str.lower() == \"yes\"]\n",
    "\n",
    "# Get a set of all individuals involved in at least one 416.bis crime\n",
    "individuals_416bis = set(sum(df_416bis[\"individuals\"], []))\n",
    "\n",
    "# Add a column to df_names indicating if the individual committed at least one 416.bis crime\n",
    "df_names[\"committed_416bis\"] = df_names[\"name\"].apply(lambda x: x in individuals_416bis)\n",
    "\n",
    "print(len(individuals_416bis), individuals_416bis)\n",
    "\n",
    "df_names = df_names[(df_names['name']!= 'NA') & ~df_names['name'].str.contains('stralciata')]\n",
    "df_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original document\n",
    "doc = Document(word_template_path)\n",
    "\n",
    "for paragraph in doc.paragraphs:\n",
    "    if '############################## ADD PEOPLE ################################' in paragraph.text:\n",
    "        # Clear the paragraph\n",
    "        paragraph.clear()\n",
    "        paragraph.alignment = WD_ALIGN_PARAGRAPH.LEFT\n",
    "        # Add new text\n",
    "        paragraph.add_run(', '.join(f'{i+1}) {person}' for i, person in enumerate(df_names['name'].tolist())))\n",
    "\n",
    "    elif '############################## ADD 416bis PEOPLE ###############################' in paragraph.text:\n",
    "        # Clear the paragraph\n",
    "        paragraph.clear()\n",
    "        paragraph.alignment = WD_ALIGN_PARAGRAPH.LEFT\n",
    "        # Add new text\n",
    "        paragraph.add_run(', '.join(f'{i+1}) {person}' for i, person in enumerate(df_names[df_names['committed_416bis']]['name'].tolist())))\n",
    "\n",
    "\n",
    "# Save the document to a new file\n",
    "doc.save(word_output_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
